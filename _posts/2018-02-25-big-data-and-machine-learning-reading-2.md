---
layout: post
title: 《白话大数据与机器学习》读书笔记2
key: 20180225
tags:
  - 大数据
  - 机器学习
  - 读书笔记
  - 信息
lang: zh-Hans
---

# 《白话大数据与机器学习》读书笔记2

## 信息论

### 信息

信息是被消除的不确定性

### 信息量

$$I=\log_2m$$

*对数*：如果$a$的$x$次方等于$N$（a>0，且a不等于1），那么数$x$叫做以$a$为底$N$的对数（logarithm），记作$x=\log_aN$。其中，a叫做对数的底数，N叫做真数。

事件出现的概率越小，信息量越大，即信息量的多少是与事件发生频率程度大小（即概率大小）恰好相反的。公式如下：

$$H(X_i)=-\log_2P$$

### 香农公式

$$C=B\cdot \log_2\left(1+\frac{S}{N}\right)$$

单位 bps

其中：

* B 是码元速率的极限值 （B=2H，H 为信道带宽，单位为 Baud）
* S 是信号功率（瓦）
* N 是噪声功率（瓦）

### 熵

#### 热力熵

#### 信息熵

$$H(x)=-\sum_{i=1}^np(x_i)\log_2P(x_1)，i=1,2,……,n$$

其中，$x$可以当成一个向量，就是若干个$x_i$产生的概率乘以该可能性的信息量，然后各项做加和。


> 信息越确定，越单一，信息熵越小；
>
> 信息越不确定，越混乱，信息熵越大。 



