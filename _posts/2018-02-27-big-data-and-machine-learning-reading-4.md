---
layout: post
title: 《白话大数据与机器学习》读书笔记4
key: 20180227
tags:
  - 大数据
  - 机器学习
  - 读书笔记
  - 分类
lang: zh-Hans
---

# 《白话大数据与机器学习》读书笔记4

## 分类

分类算法是机器学习的一个重点，也就是“监督学习”。
<!--more-->
分类和回归有相似之处：

>因变量是定量型的归纳学习称为回归，或者说是连续变量预测。<br/>
>因变量是定性型的归纳学习称为分类，或者说是离散变量预测。


### 朴素贝叶斯（Naive Bayessian）

基本思想如下：
1. 已知类条件概率密度参数表达式和先验概率。
2. 利用贝叶斯公式转成后验概率。
3. 根据后验概率大小进行决策分类。

***贝叶斯公式***：

$$P(A|B)P(B)=P(B|A)P(A)$$

$P(A)$叫做A时间先验概率，就是一般情况下，认为A发生的概率。<br/>
$P(B|A)$叫做似然率，是A假设条件成立的情况下发生B的概率。<br/>
$P(A|B)$叫做后验概率，在B发生情况下发生A的概率，也就是要计算的概率。<br/>
$P(B)$叫做标准化常量，和A的先验概率定义类似，就是一般情况下，B的发生概率。


在Pythondescikit-learn库中封装了以下类别的贝叶斯算法：
1. 高斯朴素贝叶斯（Gaussian Naive Bayes）
2. 多项式朴素贝叶斯（Multinomial Naive Bayes）
3. 伯努利朴素贝叶斯（Bernoulli Naive Bayes）


### 决策树归纳

#### 信息增益

整个样本集合的熵如下：

$$Info=-\sum_{i=1}^mp_i\log_2p_i$$

$m$的数量就是最后分类的种类，$p_i$指的实际是这个决策项产生的概率。

这个熵有另外一个叫法：期望信息。

>熵越大说明信息混乱程度越高，做切割时越复杂，要切割若干次才能完成；<br/>
>熵越小说明信息混乱程度越低，做切割时越容易，切割次数也就越少。

用某一个字段A来划分，这种划分规则下的熵为：

$$Info_A=-\sum_{j=1}^vp_j\cdot Info(A_j)$$

式中，$Info_A$是指要求的熵。$v$表示一共划分为多少组，$P_j$表示这种分组产生的概率。

### 随机森林

### 隐马尔可夫模型（Hidden Markov Model,HMM）

在语言识别，自然语言处理以及生物信息等领域体现很大的价值。

隐马尔可夫链就是贝叶斯信念网络的一种特例。

隐含状态和可见状态之间有一个概率叫做输出概率（Emission Probability）。

和HMM模型相关的算法主要分为3类：
* 问题1：知道隐含状态数量，转换概率，可见状态链，想知道隐含状态链。<br/>
  这个问题在语音识别领域叫做解码问题，其中一种解法是求最大似然状态路径。
* 问题2：知道隐含状态数量，转换概率，可见状态链，想知道可见状态的概率。
* 问题3：知道隐含状态数量，不知道转换概率，知道可见状态链，反推转换概率。

#### 维特比算法（Viterbi algorithm）

维特比算法的提出者是安德鲁·维特比，美籍犹太人，高通首席科学家，高通公司创始人之一。

这个算法研究的是一种链的可能性问题。现在应用最广的领域是CDMA通信以及打字提示功能。

CDMA协议(Code Division Multiple Access)，码分多址。


#### 前向算法（Forward Algorithm）

### 支持向量机SVM（Support Vector Machine)

支持向量机SVM是一种比较抽象的算法概念，可以用做模式识别、分类或者回归的机器学习。

#### N维空间的距离

二维空间中，一个点$(x_0,y_0)$的到一条直线$Ax+By+C=0$的距离如下：

$$d=\frac{|Ax_0+By_0+C|}{\sqrt{A^2+B^2}}$$

***这里说一下范数***

各维度空间的范数，记做$||w||$。

欧几里得范数--Euclid范数（常用计算向量长度），即向量元素绝对值的平方和再开方：

$$||x||_2=\sqrt{\sum_{i=1}^nx_i^2}$$

这样一来，距离公式就简化成为：

$$d=\frac{1}{||w||}\cdot |g(v)|$$

