---
layout: post
title: 《白话大数据与机器学习》读书笔记3
key: 20180226
tags:
  - 大数据
  - 机器学习
  - 读书笔记
  - 聚类
lang: zh-Hans
---

# 《白话大数据与机器学习》读书笔记3

## 聚类(Clustering)

聚类指的是一种学习方式，即把物理或者抽象对象的集合分组为由彼此类似的对象组成的多个类的分析过程。

### K-Means算法

步骤：
1. 从n个向量对象任意选择k个向量作为初始聚类中心。
2. 根据在步骤1中设置的k个向量（中心对象向量），计算每个对象与这k个中心对象各自的距离。
3. 对于步骤2中的计算，任何一个向量与这k个向量都有一个距离，有的远有的近，把这个向量和距离它最近的中心向量对象归在一个类簇中。
4. 重新计算每个类簇的中心对象向量位置。
5. 重复3 4两个步骤，直到类簇聚类方案中的向量归类变化极少为止。

### 有趣模式

如果一个模式具备以下特点，那么它是有趣的（Interesting）：
1. 易于被人理解。
2. 在某种确信度上，对于新的或检验数据是有效的。
3. 是潜在有用的。
4. 是新颖的。

### 孤立点

在聚类的过程中，会常常碰到一些离主群或者离每个群都非常远的点，这种点叫做孤立点，也叫做离群点。

在银行信用卡诈骗识别，网络商城防恶意刷单中多有应用。

#### 层次聚类

两种思路
> 凝聚的层次聚类方法
>
> 分裂的层次聚类方法

凝聚的层次聚类方法可以采用K-Means算法。

分裂的层次聚类方法，在Scikit-learn库中提供了一种叫做AgglomerativeClustering的分类算法。<br/>
聚类合并遵循以下原则，即基于连接度的度量来判断是否要向上继续合并两个类簇。度量有以下3种不同策略：<br/>
> Ward策略：让所有类簇中的方差最小化。<br/>
> Maximum策略：力求将类簇之间的距离最大值最小化。<br/>
> Average linkage策略：力求将簇之间的距离的平均值最小化。

层次类聚的思路的优势更多是为层次化的可视化提供支持，在我们认识比较陌生的数据层次时候比较有帮助。此外，层次类聚的思路也可以对于人们社会活动中的一些现象进行总结，例如歌曲发布网站的推荐算法。

#### 密度聚类

密度聚类很多时候用在聚类形状不规则的情形下。

*归一化问题* 是为了解决由于维度量纲或者单位不同所产生的距离计算问题而进行的权重调整--这几乎是数据挖掘必须的工作。

由于聚类是一个非监督学习的过程，所以在聚类的过程中免不了要多尝试几次，调整参数，以找到最合理的聚类方式。

**插曲**
numpy的矩阵处理，X[:,：1]中逗号前面是行的取值范围，后面是列的取值范围。冒号最多两个，第一个冒号前面是起始，第二个是截至（不包含），第三个是步长。至于其他的继续百度吧。

#### 聚类评估

评估包括以下3个方面：
1. 估计聚类的趋势。
2. 确定数据集中的簇数。
3. 测量聚类的质量。

##### 聚类趋势

用霍普金斯统计量进行量化评估：

第一步，从所有样本向量中随机找n个向量，称作$p$向量。对每一个向量都在样本空间中找一个离其最近的向量，然后求距离（用欧式距离即可），然后用$x_n$表示这个距离。

第二步，从所有样本向量中随机找n个向量，称作$q$向量。对每一个向量都在样本空间中找一个离其最近的向量，然后求距离（用欧式距离即可），然后用$y_n$表示这个距离。

第三步，求出霍普金斯统计量H：

$$H=\frac{\sum_{i=1}^n{y_i}}{\sum_{i=1}^n{x_i}+\sum_{i=1}^n{y_i}}$$

如果整个样本空间是一个均匀的，没有聚类趋势的空间，那么H应该为0.5左右。反之，如果是有聚类趋势的空间，那么H应该接近于1。

#### 簇族确定

"肘方法"（The Elbow Method），更为科学。

曲线立陡变平滑的位置附近。

#### 测定聚类质量

"内在方法"使用轮廓系数（Silhouette Coefficient）进行度量。

$$s(v)=\frac{b(v)-a(v)}{\max[a(v),b(v)]}$$

