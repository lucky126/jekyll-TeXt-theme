---
layout: post
title: 机器学习中的基本数学知识
key: 20180228
tags:
  - 机器学习
  - 数学
  - 概率
  - 统计
lang: zh-Hans
---

# 机器学习中的基本数学知识

## 线性代数（linear algebra）

$$f(x)=w^Tx+b$$

这是在机器学习中，最常见的公式。我把这个称为机器学习的第一公式，实际上就是线性分类函数(linear classifier)。<br/>
训练分类器的目标就是求出(w,b)。<br/>
其中：<br/>
$x$ 是一个一行矩阵 $[[x_1,x_2,{\cdots},x_n]]$。<br/>
$w $是一个一行矩阵 $[[w_1,w_2,{\cdots},w_n]]$。<br/>
$x$ 和 $w$ 的维度相同。<br/>
$b$ 是一个数。<br/>
$xw^T=\sum_{i=1}^nx_iw_i$，称为点积(dot product)。<br/>

>注：这里$w$表示为一个一维数组（或者向量、矢量（vector）） $[[x_1,x_2,{\cdots},x_n]]$。<br/>
>注：一维数组：在数学上，可以理解为向量，表示多维空间上的一个点。<br/>
>注：由于在线性代数中，矩阵乘法$ab≠ba$，所以对于表达式$w^Tx$，严格地说，要把矢量（向量）看做一列的矩阵(而不是一行的矩阵)，才符合数学上的定义。<br/>
>注：表达式$\vec w$ $\vec x$和$wx$是正确的，因为$w$和$x$是矢量，这个符合矢量计算的定义。<br/>

## 矩阵的操作

### 换位(transpose)

矩阵的换位操作：将矩阵中的数按照对角线交换。
数学公式：$w^T$

代码示例：
``` python
# Matrix Transpose
m = numpy.mat([[1, 2], [3, 4]])
print("Matrix.Transpose:")
print(m.T)
''' Output:
Matrix.Transpose:
[[1 3]
 [2 4]]
'''
```

### 矩阵乘法

* 矩阵相乘的含义

$$\begin{bmatrix}
x_1&x_2
\end{bmatrix}\begin{bmatrix}
y_1\\
y_2
\end{bmatrix}=x_1\times y_1+x_2\times y_2$$

我们可以看出矩阵相乘的约束：<font color=red>乘数1的列数要和乘数2的行数相等。</font>

* 矩阵乘法不满足交换律

$$m1\cdot m2\not= m2\cdot m1$$

我们再看看交换乘数后，计算的结果：

$$\begin{bmatrix}
y_1\\
y_2
\end{bmatrix}\begin{bmatrix}
x_1&x_2
\end{bmatrix}=\begin{bmatrix}
x_1\times y_1&x_1\times y_2\\
x_2\times y_1&x_2\times y_2
\end{bmatrix}$$


* 矩阵相乘是：用矩阵1的每一行和矩阵2的每一列的点积，得到一个矩阵。
$l\times m$ 的矩阵乘以 $m\times n$ 的矩阵，形成一个$l\times n$ 的矩阵。

$$
x\cdot y=\begin{bmatrix}
x_1&\cdots&x_n\\
\end{bmatrix}\begin{bmatrix}
y_1\\
\ldots\\
y_n
\end{bmatrix}=\begin{bmatrix}\sum_{i=1}^nx_iy_i\end{bmatrix}
$$<br/>
$$
x\cdot y=\begin{bmatrix}
x_1\\
\cdots\\
x_m
\end{bmatrix}\begin{bmatrix}
y_1&\cdots&y_n
\end{bmatrix}=\begin{bmatrix}
x_1y_1&\cdots&x_1y_n\\
\cdots&\cdots&\cdots\\
x_my_1&\cdots&x_my_n
\end{bmatrix}
$$<br/>
$$
x\cdot y=\begin{bmatrix}
x_{11}&\cdots&x_{1n}\\
x_{21}&\cdots&x_{2n}\\
\cdots&\cdots&\cdots\\
x_{m1}&\cdots&x_{mn}
\end{bmatrix}\begin{bmatrix}
y_{11}&\cdots&y_{1q}\\
y_{21}&\cdots&y_{2q}\\
\cdots&\cdots&\cdots\\
y_{n1}&\cdots&y_{nq}
\end{bmatrix}=\begin{bmatrix}
\sum_{i=1}^nx_{1i}y_{i1}&\cdots&\sum_{i=1}^nx_{1i}y_{iq}\\
\cdots&\cdots&\cdots\\
\sum_{i=1}^nx_{mi}y_{i1}&\cdots&\sum_{i=1}^nx_{mi}y_{iq}
\end{bmatrix}
$$

代码演示：
``` python
# Matrix Multiplication
print("Matrix Multiplication")
a = numpy.mat([1, 2])
b = numpy.mat([[10], [20]])
print(a * b)
print(a.T * b.T)

a = numpy.mat([[1, 2], [3, 4]])
b = numpy.mat([[10, 20], [30, 40]])
print(a * b)

''' Output:
[[50]]
[[10 20]
 [20 40]]
[[ 70 100]
 [150 220]]
'''
```

### 矩阵的各种乘积

#### 点积(dot product)

#### 内积(inner product)

矢量的降维运算，变成一个数。<br/>
矩阵的内积是每行每列的内积的矩阵。

$$xy=\langle x,y\rangle=\sum_{i=1}^nx_iy_i$$

``` python
x = numpy.array([1, 2])
y = numpy.array([10, 20])
print("Array inner:")
print(numpy.inner(x, y))
''' Output：
Array inner:
50
'''

x = numpy.mat([[1, 2], [3, 4]])
y = numpy.mat([10, 20])
print("Matrix inner:")
print(numpy.inner(x, y))
''' Output：
Matrix inner:
[[ 50]
 [110]]
'''
```

#### 外积(outer product)

矢量的升维运算， $m$维矢量和$n$维矢量的外积是$m\times n$为矩阵。<br/>
矩阵的并集运算， $a1\times a2$维矢量和$b1\times b2$维矩阵的外积是$(a1\times a2)\times (b1\times b2)$维矩阵。

$$x\otimes y = \begin{bmatrix}
x_{1}&\cdots&x_{1n}\\
x_{2}&\cdots&x_{2n}\\
\cdots&\cdots&\cdots\\
x_{m}&\cdots&x_{mn}
\end{bmatrix}\begin{bmatrix}
y_{1}&\cdots&y_{1q}\\
y_{2}&\cdots&y_{2q}\\
\cdots&\cdots&\cdots\\
y_{p}&\cdots&y_{pq}
\end{bmatrix}=\begin{bmatrix}
x_1y_1&\cdots&x_1y_{1q}&x_1y_2\cdots&x_1y_{pq}\\
\cdots&\cdots&\cdots&\cdots&\cdots&\cdots\\
x_{1n}y_1&\cdots&x_{1n}y_{1q}&x_{1n}y_2\cdots&x_{1n}y_{pq}\\
x_2y_1&\cdots&x_2y_{1q}&x_2y_2\cdots&x_2y_{pq}\\
\cdots&\cdots&\cdots&\cdots&\cdots&\cdots\\
x_{mn}y_1&\cdots&x_{mn}y_{1q}&x_{mn}y_2\cdots&x_{mn}y_{pq}
\end{bmatrix}$$

``` python
x = numpy.array([1, 3])
y = numpy.array([10, 20])
print("Array outer:")
print(numpy.outer(x, y))
''' Output：
Array outer:
[[10 20]
 [30 60]]
'''

x = numpy.mat([[1, 2], [3, 4]])
y = numpy.mat([10, 20])
print("Matrix outer:")
print(numpy.outer(x, y))
''' Output：
Matrix outer:
[[10 20]
 [20 40]
 [30 60]
 [40 80]]
'''
```

#### 元素积(element-wise product, point-wise product, Hadamard product )

$$
x\cdot y=\begin{bmatrix}x_1\cdots x_n\end{bmatrix}\begin{bmatrix}y_1\cdots y_n\end{bmatrix}=\begin{bmatrix}x_1y_1\cdots x_ny_n\end{bmatrix}
$$<br/>
$$
x\cdot y=\begin{bmatrix}x_1\cdots x_n\end{bmatrix}\begin{bmatrix}y_1\\
\cdots\\
y_m\end{bmatrix}=\begin{bmatrix}x_1y_1&\cdots&x_ny_1\\
\cdots&\cdots&\cdots\\
x_1y_m&\cdots&x_ny_m
\end{bmatrix}
$$<br/>
$$
x\cdot y=\begin{bmatrix}
x_{11}&\cdots&x_{1n}\\
\cdots&\cdots&\cdots\\
x_{m1}&\cdots&x_{mn}
\end{bmatrix}\begin{bmatrix}
y_{11}&\cdots&y_{1n}\\
\cdots&\cdots&\cdots\\
y_{m1}&\cdots&y_{mn}
\end{bmatrix}=\begin{bmatrix}
x_{11}y_{11}&\cdots&x_{1n}y_{1n}\\
\cdots&\cdots&\cdots\\
x_{m1}y_{m1}&\cdots&x_{mn}y_{mn}
\end{bmatrix}
$$

``` python
x = numpy.array([1, 3])
y = numpy.array([10, 20])
print("Array element-wise product:")
print(x * y)
''' Output：
Array element-wise product:
[10 60]
'''
```

## 低等数学

* 求总和公式

$$\sum_{i=1}^nx_i=x_1+x_2+\cdots+x_n$$

* 求总积公式

$$\prod_{i=1}^nx_i=x_1\times x_2\times \cdots\times x_n$$

## 微分（differential）

$f\prime(x)$
or partial differential in Leibniz notation:<br>

$$\frac{\partial f(x)}{\partial x}$$

$$\frac{dy}{dx}$$

or:<br/>

$$\frac{\nabla f(x)}{\nabla x}$$ 

the gradient of $f$ at $x$

含义

$$\frac{df(x)}{dx}=\lim_{h\to 0}\frac{f(x+h)-f(x)}{h}$$

>数学含义是在x点上，$f(x)$的变化除以$x$的变化。<br/>
>数学上可以认为是：斜率<br/>
>机器学习中指的是：梯度。<br/>
>计算梯度后，乘以一个比值（步长），可以得到矫正值，用于反向传播（矫正）权值。<br/>
>partial differential：偏微分，表示函数在某个维度上的微分。这时，可将其它维度看做常量。

### 法则

|法则|微分|偏微分|
| -  | -:   | -: |
|和法则(sum rule)|$(f+g)\prime=f\prime + g\prime$|$\frac{\partial (u+v)}{\partial x}=\frac{\partial u}{\partial x}+ \frac{\partial v}{\partial x}$|
|积法则(product rule)|$(f\cdot g)\prime=f\prime\cdot g +f\cdot g\prime$|$\frac{\partial (u\cdot v)}{\partial x}=u\cdot \frac{\partial v}{\partial x}+v\cdot \frac{\partial u}{\partial x}$|
|链式法则(chain rule of differentiation)|$(f(g(x)))\prime=f\prime(g(x))g\prime(x)$|$\frac{\partial z}{\partial x}=\frac{\partial z}{\partial y}\cdot \frac{\partial y}{\partial x}$|

### 常见导数公式

|$f(x)$|$f\prime(x)$|
|- | :-: |
|$ax$|$a$|
|$x^n$|$nx^{n-1}$|
|$x+c$|$1$|
|$e^x$|$e^x$|
|$\ln(x)$|$\frac{1}{x}$|
